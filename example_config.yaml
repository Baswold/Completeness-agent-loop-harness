# Completeness Loop Configuration
# ================================
# Copy this file and customize for your setup.

completeness_loop_config:
  
  # Model Configuration
  # -------------------
  # backend options: ollama, lmstudio, mlx, openai
  model:
    # For Ollama (recommended - easy setup)
    name: "devstral"           # Model name in Ollama
    backend: "ollama"          # Use Ollama backend
    # base_url: "http://localhost:11434"  # Default Ollama URL
    
    # For LM Studio (great GUI)
    # name: "local-model"      # LM Studio uses loaded model
    # backend: "lmstudio"
    # base_url: "http://localhost:1234"  # Default LM Studio URL
    
    # For MLX (native Apple Silicon)
    # name: "mistralai/Devstral-Small-2-24B-Instruct"
    # backend: "mlx"
    
    max_tokens: 4096
    temperature: 0.7
    base_url: null
  
  # Limits
  # ------
  limits:
    max_iterations: 50         # Max cycles before stopping
    max_runtime_hours: 12      # Max hours to run
    max_commits: 200           # Max git commits
    completion_threshold: 95   # Score needed to mark complete
  
  # Agent Prompts (optional)
  # ------------------------
  # Provide custom prompt files if you want to customize agent behavior
  agents:
    agent1_system_prompt: null  # Path to custom Agent 1 prompt file
    agent2_system_prompt: null  # Path to custom Agent 2 prompt file
    agent1_context_token_limit: 32000
    agent2_context_token_limit: 32000
  
  # Monitoring
  # ----------
  monitoring:
    log_level: "INFO"
    token_tracking: true
    log_file: "completeness_loop.log"


# Quick Setup Examples:
# ====================
#
# 1. Ollama Setup (macOS):
#    brew install ollama
#    ollama pull devstral
#    ollama serve
#
# 2. LM Studio Setup:
#    - Download from https://lmstudio.ai
#    - Load a model (Devstral, CodeLlama, DeepSeek Coder)
#    - Start local server (port 1234)
#
# 3. MLX Setup (Apple Silicon only):
#    pip install mlx-lm
#    # Model downloads automatically on first run
